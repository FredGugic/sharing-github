{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\"><u><h1>PHASE 1: AWS S3 Preparatory Work:</h1></u></font>\n",
    "<ul>\n",
    "    <li><b>Connect to AWS S3<b></li>\n",
    "    <li><b>Create a bucket in S3 to house our original images and their blurred counterparts</b></li>\n",
    "    <li><b>Upload sample images (mix of jpeg and png formats which are the only 2 formats that AWS Rekognition can work with)</b></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to AWS S3 in London\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3',region_name = 'eu-west-2') #London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Location': 'http://videre-oculo.s3.amazonaws.com/',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '0',\n",
      "                                      'date': 'Sun, 19 Jul 2020 08:27:31 GMT',\n",
      "                                      'location': 'http://videre-oculo.s3.amazonaws.com/',\n",
      "                                      'server': 'AmazonS3',\n",
      "                                      'x-amz-id-2': '7ZN2YmbXsVXZlh6rkCKHP3La+TKNs6jTIsl7W7F7OlKPshfGnduwYR7ou1TwPdpzplD+DCX5kQs=',\n",
      "                                      'x-amz-request-id': '1Z3XEWBPAZBK1K7T'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'HostId': '7ZN2YmbXsVXZlh6rkCKHP3La+TKNs6jTIsl7W7F7OlKPshfGnduwYR7ou1TwPdpzplD+DCX5kQs=',\n",
      "                      'RequestId': '1Z3XEWBPAZBK1K7T',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Create 'videre-oculo' bucket\n",
    "from pprint import pprint  # Dictionary response is better displayed...\n",
    "\n",
    "try:\n",
    "    response = s3.create_bucket(Bucket='videre-oculo', CreateBucketConfiguration={'LocationConstraint': 'eu-west-2'})\n",
    "    pprint(response)\n",
    "except Exception as e:\n",
    "    print('Exception:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h2>Upload local files to bucket</h2>\n",
    "\n",
    "<b><u>Note:</u> Not strictly required since I'll be passing my local images to Rekognition (rather than images in an S3 bucket) but shows how I would do it.<br> Also, I'm only uploading a handful of jpeg and png images for the sake of this demonstration.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded Les2Alpes.jpg\n",
      "Uploaded Hat.jpg\n",
      "Uploaded Cinema.png\n",
      "Uploaded ChristmasGames.jpg\n",
      "Uploaded PlayTime.png\n",
      "Uploaded Holidays.png\n",
      "Uploaded Cooking.png\n",
      "Uploaded Shopping.png\n",
      "Uploaded IceRink.jpg\n",
      "Uploaded Chatting.jpg\n",
      "Uploaded SatOnSofa.jpg\n"
     ]
    }
   ],
   "source": [
    "# Upload local images to new bucket (all within the 5MB limit that Rekognition imposes)\n",
    "import os\n",
    "\n",
    "local_image_folder = '/Users/fredericgugic/Oculo/Faces'\n",
    "try:\n",
    "    for file in os.listdir(local_image_folder):\n",
    "        # Ignore the folder's .DS_Store file. Don't need that!?!\n",
    "        if file.startswith('.'):\n",
    "            continue\n",
    "        # Get file extension (will end up either jpeg/jpg or png)\n",
    "        ext = file.split('.')[-1]\n",
    "        # Upload local file to bucket\n",
    "        s3.upload_file(Filename=os.path.join(local_image_folder,file), Bucket='videre-oculo',\n",
    "                       Key=f'originals/{file}',\n",
    "                       ExtraArgs={'ContentType':f'image/{ext}'})\n",
    "        print('Uploaded', file)\n",
    "except Exception as e:\n",
    "    print('Exception:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font color=\"orange\"><u><h1>PHASE 2: More preparatory work:</h1></u></font>\n",
    "<ul><li><b>Make sure all local jpeg/jpg images are stored locally with the right orientation.</li></ul>\n",
    "This is to ensure that the 'mapping' between bounding box ratios/coordinates returned by Rekognition and actual source image is the same so that blurring can be applied to the right sections of an image.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PIL to get access to the EXIF orientation details of jpeg images \n",
    "# so that they can be correctly oriented before the face detection process\n",
    "from PIL import Image\n",
    "\n",
    "# Dictionary of orientations and corresponding correction to help restore the proper orientation\n",
    "actions = {2: Image.FLIP_TOP_BOTTOM, 3: Image.ROTATE_180, 4: Image.ROTATE_180, 5: Image.ROTATE_270, \n",
    "           6: Image.ROTATE_270,      7: Image.ROTATE_90,  8: Image.ROTATE_90}\n",
    "\n",
    "# Go through local images \n",
    "for file in os.listdir(local_image_folder):\n",
    "    try:\n",
    "        # Get file extension\n",
    "        ext = file.split('.')[-1]\n",
    "        # Only deal with jpeg/jpg files\n",
    "        if ext.lower() not in ['jpeg', 'jpg']:\n",
    "            continue\n",
    "        file_path = os.path.join(local_image_folder,file)\n",
    "        # Get local image\n",
    "        local_image = Image.open(file_path)\n",
    "        # Get orientation of image from EXIF details (key 274). Default to 1 when no EXIF details exist.\n",
    "        orientation = dict(local_image.getexif()).get(274, 1)\n",
    "        # Rotate/Flip the image as necessary\n",
    "        if orientation in [2, 3, 6, 8]:\n",
    "            local_image = local_image.transpose(actions[orientation])\n",
    "        if orientation in [4, 5, 7]:\n",
    "            local_image = local_image.transpose(actions[orientation]).transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        # Save correctly oriented image back to local folder\n",
    "        local_image.save(f'/Users/fredericgugic/Oculo/Faces/{file}')\n",
    "    except Exception as e:\n",
    "        print(e)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font color=\"orange\"><u><h1>PHASE 3: AWS Face Detection With Rekognition:</h1></u></font>\n",
    "<ul>\n",
    "    <li><b>Connect to AWS Rekognition<b></li>\n",
    "    <li><b>Loop through local images</b></li>\n",
    "        <ol>\n",
    "            <li>Send image to Rekognition for face detection</li>\n",
    "            <li>Loop through all detected faces within an image</li>\n",
    "                <ul>\n",
    "                    <li>Use ratios in BoundingBox dictionary to derive actual pixel coordinates</li>\n",
    "                    <li>Apply Gaussian filter to blur face within bounding box</li>\n",
    "                    <li>Outline box</li>\n",
    "                </ul>\n",
    "            <li>Upload blurred image to 'blurred' folder in S3 bucket</li>\n",
    "        </ol>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to AWS Rekognition in London \n",
    "rekog = boto3.client('rekognition', region_name = 'eu-west-2')             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io, filters\n",
    "\n",
    "# Go through local images and carry out blurring process \n",
    "for file in os.listdir(local_image_folder):\n",
    "    try:\n",
    "        # Ignore the folder's .DS_Store file!!\n",
    "        if file.startswith('.'):\n",
    "            continue\n",
    "        file_path = os.path.join(local_image_folder,file)\n",
    "        # Get file extension\n",
    "        ext = file.split('.')[-1]\n",
    "        \n",
    "        # Store image as ndarray in preparation for processing sections where faces are detected\n",
    "        if ext.lower() in ['jpeg', 'jpg']:  # No Alpha component for jpeg/jpg, only keep RGB components\n",
    "            work_image = io.imread(file_path)[:,:,:3]  # RGB\n",
    "        else:\n",
    "            work_image = io.imread(file_path)  # RGBA for png images\n",
    "        # Get image size in pixels(height H and width W)\n",
    "        H, W = work_image.shape[:2]\n",
    " \n",
    "        # Let AWS Rekognition detect faces in our local image \n",
    "        with open(file_path, 'rb') as the_image:\n",
    "            faces = rekog.detect_faces(Image={'Bytes': the_image.read()})\n",
    "        \n",
    "        # Fit each blurred face within a white 1px outline rectangle so that it stands out a bit more \n",
    "        for face in faces[\"FaceDetails\"]:\n",
    "            # Values in dict BoundingBox are ratios so have to be multiplied by H or W to get pixels back\n",
    "            # X and Y are the coordinates of the top left corner of the bounding box\n",
    "            X = int(face[\"BoundingBox\"][\"Left\"] * W)\n",
    "            Y = int(face[\"BoundingBox\"][\"Top\"] * H)\n",
    "            # Similarly, derive the width and height of the bounding box\n",
    "            box_width = int(face[\"BoundingBox\"][\"Width\"] * W)\n",
    "            box_height = int(face[\"BoundingBox\"][\"Height\"] * H)\n",
    "            # Taking into account the outline rectangle we want to achieve, here's what we do:\n",
    "            # - Adjust the bounding box boundaries to get a 'framed' copy of the detected face\n",
    "            detected_face = work_image[Y+1:Y+box_height-1, X+1:X+box_width-1,:3]\n",
    "            # - Use a Gaussian filter to blur the face\n",
    "            #  (get more 'responsive' sigma value depending on the ratio of areas of whole image to bounding box)\n",
    "            dynamic_sigma = 15 + (H * W) / (box_height * box_width * 2)\n",
    "            blurred_face = filters.gaussian(detected_face, multichannel=True, sigma=dynamic_sigma) * 256\n",
    "            # - Fill the whole of the bounding box white\n",
    "            work_image[Y:Y+box_height, X:X+box_width,:3] = 255\n",
    "            # - Overlay blurred face to white box, achieving the 'framing'\n",
    "            work_image[Y+1:Y+box_height-1, X+1:X+box_width-1,:3] = blurred_face\n",
    "            \n",
    "        #Save blurred image locally and also upload to bucket \n",
    "        io.imsave(f'/Users/fredericgugic/Oculo/Blurred/{file}', work_image)\n",
    "        s3.upload_file(Filename=f'/Users/fredericgugic/Oculo/Blurred/{file}', Bucket='videre-oculo',\n",
    "                       Key=f'blurred/{file}',\n",
    "                       ExtraArgs={'ContentType':f'image/{ext}'})\n",
    "    except Exception as e:\n",
    "        print(e)              "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
